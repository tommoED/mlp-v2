{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! A large language model (LLM) refers to an artificial intelligence system that can generate human-like text based on the input it receives. These models have been trained on vast amounts of data and can perform tasks such as generating code, writing articles, composing emails, and even playing games.\\n\\nLarge language models differ from traditional machine learning algorithms in several ways:\\n\\n1. **Training Data**: They use much larger datasets than traditional neural networks.\\n2. **Complexity**: LLMs often have more layers and deeper architectures compared to simpler neural networks.\\n3. **Generative Power**: They can produce natural-sounding text with varying levels of complexity and style.\\n4. **Versatility**: They can be used for various applications beyond just generating text, such as summarizing documents, translating languages, or performing complex calculations.\\n\\nOne notable example of a large language model is AlphaGo, developed by Google AI, which defeated the world champion of Go against Lee Sedol using sophisticated strategies inspired by deep reinforcement learning and generative adversarial networks.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Generate the following ics calendar variables for a given time. \n",
    "Example: \"tomorrow morining, Today is 20240311T124823\"\n",
    "Output:\n",
    "DTSTART: 20240312T090000\n",
    "DTEND: 20240312T100000\n",
    "\n",
    "Prompt: \"yesterday evening Today is 20250311T124823\"\n",
    "Output:\n",
    "'''\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, You always follow the example.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTSTART: 20250310T210000\n",
      "DTEND: 20250310T220000\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mantas/miniconda3/envs/mlp_practical/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Converting and de-quantizing GGUF tensors...: 100%|██████████| 291/291 [00:23<00:00, 12.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct-GGUF\"\n",
    "filename = \"qwen2.5-0.5b-instruct-q8_0.gguf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
